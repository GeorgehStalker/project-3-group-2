{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the bear attack dataset\n",
    "bear_data = pd.read_csv(\"C:/Users/georg/git repositories/project 3 group 2/project-3-group-2/eda/Resources/bear_attacks.csv\")\n",
    "# Load the shark attack dataset\n",
    "shark_data = pd.read_excel(\"C:/Users/georg/git repositories/project 3 group 2/project-3-group-2/eda/Resources/GSAF5-1.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CaseNumber', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
      "       'Activity', 'Name', 'Sex', 'Age', 'Injury', 'Fatal(Y/N)', 'Time',\n",
      "       'Species', 'InvestigatororSource', 'pdf', 'hrefformula', 'href',\n",
      "       'CaseNumber.1', 'CaseNumber.2', 'originalorder', 'Unnamed:22',\n",
      "       'Unnamed:23'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Remove spaces from column headers and replace with underscores\n",
    "shark_data.columns = shark_data.columns.str.replace(' ' ,'')\n",
    "\n",
    "# Display updated column names\n",
    "print(shark_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['27-Aug-2019' '21-Aug-2019' '20-Aug-2019' ... 'Reported 08-Jan-2017'\n",
      " 'Reported 19-Aug-1836' nan]\n"
     ]
    }
   ],
   "source": [
    "# Verify the original values\n",
    "distinct_values = shark_data['Date'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' nan 'Y' 'M' 'UNKNOWN' 2017 'Y x 2' ' N' 'N ' 'y']\n"
     ]
    }
   ],
   "source": [
    "# Verify the original values\n",
    "distinct_values = shark_data['Fatal(Y/N)'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' nan 'Y']\n"
     ]
    }
   ],
   "source": [
    "# Define mapping for Fatal(Y/N) values\n",
    "def standardize_fatal(value):\n",
    "    value = str(value).strip().upper()\n",
    "    if value in ['Y', 'YES', 'Y X 2']:\n",
    "        return 'Y'\n",
    "    elif value in ['N', 'NO', 'N ']:\n",
    "        return 'N'\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the 'Fatal(Y/N)' column\n",
    "shark_data['Fatal(Y/N)'] = shark_data['Fatal(Y/N)'].apply(standardize_fatal)\n",
    "\n",
    "# Verify the updated values\n",
    "distinct_values = shark_data['Fatal(Y/N)'].unique()\n",
    "print(distinct_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatetimeArray>\n",
      "['2019-08-27 00:00:00', '2019-08-21 00:00:00', '2019-08-20 00:00:00',\n",
      " '2019-08-16 00:00:00', '2019-08-11 00:00:00', '2019-08-04 00:00:00',\n",
      " '2019-08-03 00:00:00', '2019-07-30 00:00:00', '2019-07-29 00:00:00',\n",
      " '2019-07-28 00:00:00',\n",
      " ...\n",
      " '1783-03-02 00:00:00', '1780-08-08 00:00:00', '1771-07-12 00:00:00',\n",
      " '1753-10-27 00:00:00', '1751-07-27 00:00:00', '1742-12-17 00:00:00',\n",
      " '1738-04-06 00:00:00', '1703-03-26 00:00:00', '2017-01-08 00:00:00',\n",
      " '1836-08-19 00:00:00']\n",
      "Length: 4738, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Reported' with an empty string in the 'Date' column\n",
    "shark_data['Date'] = shark_data['Date'].str.replace('Reported ', '', regex=False)\n",
    "\n",
    "# Convert 'Date' to datetime\n",
    "shark_data['Date'] = pd.to_datetime(shark_data['Date'], errors='coerce', format='%d-%b-%Y')\n",
    "\n",
    "# Optionally, print the distinct values after cleaning\n",
    "distinct_values = shark_data['Date'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019. 2018. 2017. 2016. 2015. 2014. 2013. 2012. 2011. 2010. 2009. 2008.\n",
      " 2007. 2006. 2005. 2004. 2003. 2002. 2001. 2000. 1999. 1998. 1997. 1996.\n",
      " 1995. 1994. 1993. 1992. 1991. 1990. 1989. 1988. 1987. 1986. 1985. 1984.\n",
      " 1983. 1982. 1981. 1980. 1979. 1978. 1977. 1976. 1975. 1974. 1973. 1972.\n",
      " 1971. 1970. 1969. 1968. 1967. 1966. 1965. 1964. 1963. 1962. 1961. 1960.\n",
      " 1959. 1958. 1957. 1956. 1955. 1954. 1953. 1952. 1951. 1950. 1949. 1948.\n",
      " 1947. 1946. 1945. 1944. 1943. 1942. 1941. 1940. 1939. 1938. 1937. 1936.\n",
      " 1935. 1934. 1933. 1932. 1931. 1930. 1929. 1928. 1927. 1926. 1925. 1924.\n",
      " 1923. 1922. 1921. 1920. 1919. 1918. 1917. 1916. 1915. 1914. 1913. 1912.\n",
      " 1911. 1910. 1909. 1908. 1907. 1906. 1905. 1904. 1903. 1902. 1901. 1900.\n",
      " 1899. 1898. 1897. 1896. 1895. 1894. 1893. 1892. 1891. 1890. 1889. 1888.\n",
      " 1887. 1886. 1885. 1884. 1883. 1882. 1881. 1880. 1879. 1878. 1877. 1876.\n",
      " 1875. 1874. 1873. 1872. 1871. 1870. 1869. 1868. 1867. 1866. 1865. 1864.\n",
      " 1863. 1862. 1861. 1860. 1859. 1858. 1857. 1856. 1855. 1853. 1852. 1851.\n",
      " 1850. 1849. 1848. 1847. 1846. 1845. 1844. 1842. 1841. 1840. 1839. 1837.\n",
      " 1836. 1835. 1834. 1832. 1831. 1830. 1829. 1828. 1827. 1826. 1825. 1823.\n",
      " 1822. 1819. 1818. 1817. 1816. 1815. 1812. 1811. 1810. 1808. 1807. 1805.\n",
      " 1804. 1803. 1802. 1801. 1800. 1797. 1792. 1791. 1788. 1787. 1786. 1785.\n",
      " 1784. 1783. 1780. 1779. 1776. 1771. 1767. 1764. 1758. 1755. 1753. 1751.\n",
      " 1749. 1748. 1742. 1738. 1733. 1723. 1721. 1703. 1700. 1642. 1638. 1637.\n",
      " 1617. 1595. 1580. 1555. 1554. 1543.  500.   77.    5.    0.   nan]\n"
     ]
    }
   ],
   "source": [
    "# Verify the original values\n",
    "distinct_values = shark_data['Year'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019. 2018. 2017. 2016. 2015. 2014. 2013. 2012. 2011. 2010. 2009. 2008.\n",
      " 2007. 2006. 2005. 2004. 2003. 2002. 2001. 2000. 1999. 1998. 1997. 1996.\n",
      " 1995. 1994. 1993. 1992. 1991. 1990. 1989. 1988. 1987. 1986. 1985. 1984.\n",
      " 1983. 1982. 1981. 1980. 1979. 1978. 1977. 1976. 1975. 1974. 1973. 1972.\n",
      " 1971. 1970. 1969. 1968. 1967. 1966. 1965. 1964. 1963. 1962. 1961. 1960.\n",
      " 1959. 1958. 1957. 1956. 1955. 1954. 1953. 1952. 1951. 1950. 1949. 1948.\n",
      " 1947. 1946. 1945. 1944. 1943. 1942. 1941. 1940. 1939. 1938. 1937. 1936.\n",
      " 1935. 1934. 1933. 1932. 1931. 1930. 1929. 1928. 1927. 1926. 1925. 1924.\n",
      " 1923. 1922. 1921. 1920. 1919. 1918. 1917. 1916. 1915. 1914. 1913. 1912.\n",
      " 1911. 1910. 1909. 1908. 1907. 1906. 1905. 1904. 1903. 1902. 1901. 1900.\n",
      "   nan]\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Year' to numeric, setting errors='coerce' to handle non-numeric values\n",
    "shark_data['Year'] = pd.to_numeric(shark_data['Year'], errors='coerce')\n",
    "\n",
    "# Define valid range for years\n",
    "valid_year_range = (1900, 2023)  # You can adjust the range as needed\n",
    "\n",
    "# Filter out invalid years outside the valid range\n",
    "shark_data['Year'] = shark_data['Year'].apply(lambda x: x if (pd.notnull(x) and valid_year_range[0] <= x <= valid_year_range[1]) else np.nan)\n",
    "\n",
    "# Print the distinct values after cleaning\n",
    "distinct_values = shark_data['Year'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40 11 26 9 16 51 21 20 37 nan 18 49 23 45 \"20's\" 43 32 50 8 64 19 17 65\n",
      " 10 67 53 28 12 25 58 74 46 41 31 15 '9 & 60' 35 48 24 36 '20s' 42 33 29\n",
      " 55 13 39 '60s' 14 56 61 'a minor' 6 62 30 57 52 34 60 54 69 38 '40s' 22 7\n",
      " 3 82 73 68 'Teen' 47 66 72 59 27 71 44 '38' '39' '23' '32' '52' '68' '12'\n",
      " '18' '19' '43' '47' '6' '37' '9' '36' '10' '16' '13' '11' '17' '14' '30'\n",
      " '50' '29' '65' '63' '26' '71' '48' '70' '58' '18 months' '22' '41' '35'\n",
      " '57' '20' '24' '34' '15' '44' '53' '7' '40' '28' '33' '30s' '31' '45'\n",
      " '50s' '8' '51' '61' '42' '25' 'teen' '66' '21' '77' '46' '60' '74' '55'\n",
      " '27' '3' '56' '64' '28 & 26' '62' '5' '49' '54' '86' '59' '18 or 20'\n",
      " '12 or 13' '46 & 34' '28, 23 & 30' 'Teens' 77 63 '36 & 26' '8 or 10' 84\n",
      " '\\xa0 ' ' ' '30 or 36' '6½' '21 & ?' 75 '33 or 37' 'mid-30s' '23 & 20' 5\n",
      " ' 30' '7      &    31' ' 28' '20?' \"60's\" '69' '32 & 30' '16 to 18' '87'\n",
      " '67' 'Elderly' 'mid-20s' 'Ca. 33' '74 ' '45 ' '21 or 26' '20 ' '>50'\n",
      " '18 to 22' 'adult' '9 & 12' '? & 19' '9 months' '25 to 35' '23 & 26' 1\n",
      " '(adult)' '33 & 37' '25 or 28' '37, 67, 35, 27,  ? & 27' '21, 34,24 & 35'\n",
      " '30 & 32' '50 & 30' '17 & 35' 'X' '\"middle-age\"' '13 or 18' '34 & 19'\n",
      " '33 & 26' '2 to 3 months' 70 '4' 'MAKE LINE GREEN' ' 43' '81' '\"young\"'\n",
      " '7 or 8' 78 '17 & 16' 'F' 'Both 11' '9 or 10' 'young' '36 & 23' '  ' '78'\n",
      " 'A.M.' '?    &   14' '10 or 12' '31 or 33' '2½' '1' '13 or 14']\n"
     ]
    }
   ],
   "source": [
    "# Verify the original values\n",
    "distinct_values = shark_data['Age'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in 'Age': 22281\n",
      "[40. 11. 26.  9. 16. 51. 21. 20. 37. nan 18. 49. 23. 45. 43. 32. 50.  8.\n",
      " 64. 19. 17. 65. 10. 67. 53. 28. 12. 25. 58. 74. 46. 41. 31. 15. 35. 48.\n",
      " 24. 36. 42. 33. 29. 55. 13. 39. 14. 56. 61.  6. 62. 30. 57. 52. 34. 60.\n",
      " 54. 69. 38. 22.  7.  3. 82. 73. 68. 47. 66. 72. 59. 27. 71. 44. 63. 70.\n",
      " 77.  5. 86. 84. 75. 87.  1.  4. 81. 78.]\n"
     ]
    }
   ],
   "source": [
    "# Function to convert values to numeric, replacing invalid ones with NaN\n",
    "def convert_to_numeric(value):\n",
    "    try:\n",
    "        # Convert value to numeric, if possible\n",
    "        return pd.to_numeric(value, errors='coerce')\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the 'Age' column\n",
    "shark_data['Age'] = shark_data['Age'].apply(convert_to_numeric)\n",
    "\n",
    "# Print the number of NaN values in the 'Age' column to verify\n",
    "print(f\"Number of NaN values in 'Age': {shark_data['Age'].isna().sum()}\")\n",
    "\n",
    "# Verify the original values\n",
    "distinct_values = shark_data['Age'].unique()\n",
    "print(distinct_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'F' nan 'M ' 'lli' 'M x 2' 'N' '.']\n"
     ]
    }
   ],
   "source": [
    "# Verify the original values\n",
    "distinct_values = shark_data['Sex'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'F' 'U']\n"
     ]
    }
   ],
   "source": [
    "# Define the valid values for sex\n",
    "valid_sex_values = ['M', 'F']\n",
    "\n",
    "# Use apply with a lambda function to clean the 'Sex ' column\n",
    "shark_data['Sex'] = shark_data['Sex'].apply(lambda x: x.strip() if isinstance(x, str) else x)  # Remove any leading/trailing spaces\n",
    "shark_data['Sex'] = shark_data['Sex'].apply(lambda x: x if x in valid_sex_values else 'U')\n",
    "\n",
    "# Verify the cleaned values\n",
    "distinct_values = shark_data['Sex'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156 entries, 0 to 155\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       156 non-null    object \n",
      " 1   Location   156 non-null    object \n",
      " 2   Details    156 non-null    object \n",
      " 3   Bear       156 non-null    object \n",
      " 4   Latitude   156 non-null    float64\n",
      " 5   Longitude  156 non-null    float64\n",
      " 6   Name       155 non-null    object \n",
      " 7   Age        155 non-null    object \n",
      " 8   Gender     155 non-null    object \n",
      "dtypes: float64(2), object(7)\n",
      "memory usage: 11.1+ KB\n"
     ]
    }
   ],
   "source": [
    "bear_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['August 23, 2018' 'July 3, 2018' 'July 9, 1999' 'December 8, 1990'\n",
      " 'November 29, 1983' 'January 5, 1975' 'November 17, 1968'\n",
      " 'September 1, 2019' 'June 19, 2017' 'June 18, 2017' 'May 10, 2015'\n",
      " 'September 21, 2014' 'May 7, 2014' 'June 6, 2013' 'July 25, 2011'\n",
      " 'June 2011' 'August 7, 2009' 'May 30, 2008' 'July 20, 2007'\n",
      " 'June 17, 2007' 'April 13, 2006' 'September 6, 2005' 'August 26, 2005'\n",
      " 'June 14, 2005' 'September 29, 2002' 'September 1, 2002'\n",
      " 'August 19, 2002' 'August 18, 2001' 'June 3, 2001' 'July 2, 2000'\n",
      " 'May 21, 2000' 'August 14, 1997' 'June 14, 1996' 'September 16, 1994'\n",
      " 'August 10, 1993' 'July 8, 1992' 'June 14, 1992' 'October 11, 1991'\n",
      " 'May 26, 1991' 'May 29, 1985' 'July 6, 1983' 'May 27, 1983'\n",
      " 'May 21, 1983' 'August 14, 1980' 'July 18, 1980' 'May 13, 1978'\n",
      " 'May 16, 1974' 'July 25, 1971' 'October 1, 1968' 'August 8, 1967'\n",
      " 'July 2, 1965' 'September 17, 1964' 'August 16, 1963' 'September 6, 1959'\n",
      " 'August 12, 1958' 'November 19, 1952' 'July 7, 1948' 'November 23, 1943'\n",
      " 'November 24, 1906' 'May 19, 1901' 'September 4, 1883'\n",
      " 'December 29, 1883' 'June 1881' 'August 1784' 'August 15, 2019'\n",
      " 'November 26, 2018' 'October 1, 2018' 'September 14, 2018'\n",
      " 'June 19, 2018' 'June 29, 2016' 'August 7, 2015' 'October 14, 2014'\n",
      " 'September 17, 2014' 'September 7, 2014' 'September 4, 2014'\n",
      " 'August 24, 2012' 'October 2012' 'August 24, 2011' 'July 6, 2011'\n",
      " 'July 28, 2010' 'June 17, 2010' 'October 1, 2008' 'November 25, 2007'\n",
      " 'April 28, 2006' 'September 20, 2005' 'June 23, 2005' 'June 5, 2005'\n",
      " 'October 5, 2003' 'October 30, 2001' 'July 14, 2000' 'November 1, 1999'\n",
      " 'May 25, 1999' 'October 24, 1998' 'August 22, 1998' 'May 17, 1998'\n",
      " 'February 8, 1998' 'August 23, 1996' 'July 5, 1996' 'October 9, 1995'\n",
      " 'July 1, 1995' 'October 3, 1992' 'September 15, 1992' 'July 10, 1992'\n",
      " 'November 4, 1988' 'July 23, 1987' 'April 25, 1987' 'October 5, 1986'\n",
      " 'July 30, 1984' 'June 25, 1983' 'September 30, 1980' 'August 24, 1980'\n",
      " 'July 24, 1980' 'September 15, 1979' 'July 1, 1977' 'September 23, 1976'\n",
      " 'September 11, 12 or 13, 1976; exact date unknown' 'July 24, 1976'\n",
      " 'August 3, 1974' 'September 25, 1973' 'June 25, 1972' 'January 14, 1970'\n",
      " 'August 13, 1967' 'October 27, 1958' 'October 22, 1956'\n",
      " 'September 19, 1955' 'August 23, 1942' 'October 1932'\n",
      " 'September 12, 1929' 'June 12, 1922' 'September 8, 1916'\n",
      " 'Unknown date prior to February 1914' 'September 2, 1892' 'June 1886'\n",
      " 'July 1885' '24 June 1892' '1875' 'August 30, 1863' '1855'\n",
      " 'December 19, 1853' 'October 27, 1854' 'Circa 1850' 'October 17, 1837']\n"
     ]
    }
   ],
   "source": [
    "# Verify the original values\n",
    "distinct_values = bear_data['Date'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date    Year  Month   Day\n",
      "0 2018-08-23  2018.0    8.0  23.0\n",
      "1 2018-07-03  2018.0    7.0   3.0\n",
      "2 1999-07-09  1999.0    7.0   9.0\n",
      "3 1990-12-08  1990.0   12.0   8.0\n",
      "4 1983-11-29  1983.0   11.0  29.0\n",
      "Distinct Years: [2018. 1999. 1990. 1983. 1975. 1968. 2019. 2017. 2015. 2014. 2013. 2011.\n",
      "   nan 2009. 2008. 2007. 2006. 2005. 2002. 2001. 2000. 1997. 1996. 1994.\n",
      " 1993. 1992. 1991. 1985. 1980. 1978. 1974. 1971. 1967. 1965. 1964. 1963.\n",
      " 1959. 1958. 1952. 1948. 1943. 1906. 1901. 1883. 2016. 2012. 2010. 2003.\n",
      " 1998. 1995. 1988. 1987. 1986. 1984. 1979. 1977. 1976. 1973. 1972. 1970.\n",
      " 1956. 1955. 1942. 1929. 1922. 1916. 1892. 1863. 1853. 1854. 1837.]\n",
      "Distinct Months: [ 8.  7. 12. 11.  1.  9.  6.  5. nan  4. 10.  2.]\n",
      "Distinct Days: [23.  3.  9.  8. 29.  5. 17.  1. 19. 18. 10. 21.  7.  6. 25. nan 30. 20.\n",
      " 13. 26. 14.  2. 16. 11. 27. 12. 24.  4. 15. 28. 22.]\n",
      "<bound method Series.unique of 0       8-23-2018\n",
      "1        7-3-2018\n",
      "2        7-9-1999\n",
      "3       12-8-1990\n",
      "4      11-29-1983\n",
      "          ...    \n",
      "151           NaN\n",
      "152    12-19-1853\n",
      "153    10-27-1854\n",
      "154           NaN\n",
      "155    10-17-1837\n",
      "Length: 156, dtype: object>\n",
      "<DatetimeArray>\n",
      "['2018-08-23 00:00:00', '2018-07-03 00:00:00', '1999-07-09 00:00:00',\n",
      " '1990-12-08 00:00:00', '1983-11-29 00:00:00', '1975-01-05 00:00:00',\n",
      " '1968-11-17 00:00:00', '2019-09-01 00:00:00', '2017-06-19 00:00:00',\n",
      " '2017-06-18 00:00:00',\n",
      " ...\n",
      " '1955-09-19 00:00:00', '1942-08-23 00:00:00', '1929-09-12 00:00:00',\n",
      " '1922-06-12 00:00:00', '1916-09-08 00:00:00', '1892-09-02 00:00:00',\n",
      " '1863-08-30 00:00:00', '1853-12-19 00:00:00', '1854-10-27 00:00:00',\n",
      " '1837-10-17 00:00:00']\n",
      "Length: 130, dtype: datetime64[ns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_21152\\3798552469.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  bear_data['Date'] = pd.to_datetime(bear_data['Date'], errors='coerce', infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "# Replace specific text patterns in the 'Date' column (e.g., 'Reported ')\n",
    "bear_data['Date'] = bear_data['Date'].str.replace('Reported ', '', regex=False)\n",
    "\n",
    "# Convert 'Date' to datetime\n",
    "bear_data['Date'] = pd.to_datetime(bear_data['Date'], errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "# Function to extract year, month, and day from datetime\n",
    "def extract_date_components(date):\n",
    "    if pd.notnull(date):\n",
    "        return pd.Series([date.year, date.month, date.day])\n",
    "    else:\n",
    "        return pd.Series([np.nan, np.nan, np.nan])\n",
    "\n",
    "# Apply the function to the 'Date' column and create new columns for year, month, and day\n",
    "bear_data[['Year', 'Month', 'Day']] = bear_data['Date'].apply(extract_date_components)\n",
    "\n",
    "# Display the cleaned DataFrame with Year, Month, and Day\n",
    "print(bear_data[['Date', 'Year', 'Month', 'Day']].head())\n",
    "\n",
    "# Verify the distinct values in the new columns\n",
    "distinct_years = bear_data['Year'].unique()\n",
    "distinct_months = bear_data['Month'].unique()\n",
    "distinct_days = bear_data['Day'].unique()\n",
    "\n",
    "print(\"Distinct Years:\", distinct_years)\n",
    "print(\"Distinct Months:\", distinct_months)\n",
    "print(\"Distinct Days:\", distinct_days)\n",
    "\n",
    "date_string = bear_data['Month'].astype(str) + \"-\" + bear_data['Day'].astype(str) + \"-\" + bear_data['Year'].astype(str)\n",
    "\n",
    "date_string = date_string.replace('nan-nan-nan', np.nan)\n",
    "date_string = date_string.str.replace('.0','')\n",
    "\n",
    "print(date_string.unique)\n",
    "\n",
    "bear_data['Date'] = pd.to_datetime(date_string, errors='coerce', format='%m-%d-%Y')\n",
    "\n",
    "print(bear_data['Date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 33' ' 31' ' 64' ' 28' ' 46' ' 18' ' 19' ' 62' ' 27' ' 16' ' 22' ' 36'\n",
      " ' 61' ' 72' ' 74' ' 70' ' 11' ' 6' ' 30' ' 69' ' 71' ' 77' ' 5 months'\n",
      " ' 93' ' 24' ' 50' ' 56' ' 37' ' 53' ' 4' ' 20' ' 32' ' 48' ' 12' ' 55'\n",
      " ' 26' ' 44' ' 10' ' 15' ' 51' ' 7' ' 3' ' 5' ' 52' '\\xa0?' ' 82' '\" 8'\n",
      " ' 10 months' ' 38' ' 63' ' 42' ' 54' ' 49' ' 59' ' 57' ' 60' ' 58' ' 35'\n",
      " ' 41' ' 65' ' 40' ' 45' ' 29' ' 25' ' 23' nan ' 43' ' 68']\n"
     ]
    }
   ],
   "source": [
    "# Verify the original values\n",
    "distinct_values = bear_data['Age'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Age\n",
      "0  33.0\n",
      "1  31.0\n",
      "2  64.0\n",
      "3  28.0\n",
      "4  46.0\n",
      "Distinct Ages: [33. 31. 64. 28. 46. 18. 19. 62. 27. 16. 22. 36. 61. 72. 74. 70. 11.  6.\n",
      " 30. 69. 71. 77. nan 93. 24. 50. 56. 37. 53.  4. 20. 32. 48. 12. 55. 26.\n",
      " 44. 10. 15. 51.  7.  3.  5. 52. 82. 38. 63. 42. 54. 49. 59. 57. 60. 58.\n",
      " 35. 41. 65. 40. 45. 29. 25. 23. 43. 68.]\n"
     ]
    }
   ],
   "source": [
    "# Function to clean and convert age values\n",
    "def clean_age(value):\n",
    "    # Remove leading/trailing whitespace\n",
    "    value = str(value).strip()\n",
    "    \n",
    "    # Handle special cases\n",
    "    if 'month' in value or '?' in value or 'unknown' in value.lower():\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert to numeric, setting errors='coerce' to handle non-numeric values\n",
    "    try:\n",
    "        # Convert value to numeric, forcing errors to NaN\n",
    "        return pd.to_numeric(value, errors='coerce')\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing age: {value} - {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the 'Age' column\n",
    "bear_data['Age'] = bear_data['Age'].apply(clean_age)\n",
    "\n",
    "# Display the cleaned DataFrame with the 'Age' column\n",
    "print(bear_data[['Age']].head())\n",
    "\n",
    "# Verify the distinct values in the 'Age' column\n",
    "distinct_ages = bear_data['Age'].unique()\n",
    "print(\"Distinct Ages:\", distinct_ages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' male' ' female' nan]\n"
     ]
    }
   ],
   "source": [
    "# Verify the original values\n",
    "distinct_values = bear_data['Gender'].unique()\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'F' 'U']\n"
     ]
    }
   ],
   "source": [
    "# Define mapping for gender\n",
    "gender_mapping = {\n",
    "    ' male': 'M',\n",
    "    ' female': 'F',\n",
    "    '': 'U',  \n",
    "    np.nan: 'U'  # Handles NaN values\n",
    "}\n",
    "# Replace values in the 'Gender' column using the mapping\n",
    "bear_data['Gender'] = bear_data['Gender'].replace(gender_mapping)\n",
    "\n",
    "# Rename 'Gender' to 'Sex'\n",
    "bear_data.rename(columns={'Gender': 'Sex'}, inplace=True)\n",
    "\n",
    "# Verify the updated values\n",
    "distinct_values = bear_data['Sex'].unique()\n",
    "print(distinct_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
